5.1 CAP Theorem (Interview version)

"The CAP theorem states that in a distributed system, when a network partition occurs, a system must choose between consistency and availability, since partition tolerance is unavoidable in real-world networks. CP systems prioritize correctness by rejecting or delaying requests until replicas agree, while AP systems prioritize uptime by allowing nodes to continue accepting requests even if replicas temporarily diverge. Most large-scale internet systems lean toward AP and achieve correctness later through eventual consistency, where replicas asynchronously synchronize using techniques such as replication logs, versioning, last-write-wins, vector clocks, or application-level merge strategies. A common design approach is modeling updates as append-only events instead of overwriting shared state, allowing replicas to merge missing events and converge safely once communication is restored."

âœ… Real-World Examples (Short Add-On You Can Say)

Amazon (Shopping Cart)
Amazon models cart updates as add/remove events rather than a single mutable cart value. During partitions, different regions record actions independently, and later merge events to reconstruct the final cart without conflicts.

Netflix (Viewing Progress)
Netflix records viewing activity as events with timestamps. When multiple devices update progress independently, the system reconciles state using versioning or latest-progress logic after synchronization.

Bloomberg / Financial Systems
Market systems store immutable trade events instead of overwriting prices. After partitions, systems replay and merge trade logs to recompute accurate prices and volumes consistently.


5.2 Message Queues (Interview version)

"A message queue is a distributed infrastructure component that enables asynchronous communication between services by decoupling producers and consumers through durable message storage. Instead of services calling each other synchronously, producers publish events to a queue, and background worker services consume and process them independently, improving scalability, reliability, and fault tolerance. Message queues are widely used for tasks such as payment processing, order workflows, notifications, analytics pipelines, and event propagation in eventually consistent systems. Common implementations include task queues like RabbitMQ and AWS SQS, and distributed log-based systems like Kafka or Google Pub/Sub for high-throughput event streaming. Their advantages include load buffering during traffic spikes, failure isolation, retry mechanisms, and improved system resilience; however, they introduce challenges such as duplicate message delivery, increased system complexity, and eventual consistency concerns. Production queues prevent message loss through durable disk persistence, replication across nodes, and acknowledgement-based delivery, typically providing at-least-once delivery guarantees. Consumer workers - independent backend services - process messages and implement idempotency checks using databases to ensure operations execute safely even when retries occur. Together with retries, dead-letter queues, and backpressure handling, message queues form the backbone of reliable distributed systems by ensuring work is eventually processed even under partial failures."

5.3 Containers, Kubernetes, Throughput & Throttling (Interview version)

"In modern distributed systems, application services are typically packaged as containers using tools like Docker, which bundle the application code, runtime, and dependencies to ensure consistent execution across environments. These containers are deployed and managed by orchestration platforms such as Kubernetes, which handles scheduling, scaling, health checks, and resource allocation across a cluster of machines. Horizontal scaling at the application layer is achieved by Kubernetes creating multiple container instances (pods) behind a load balancer to increase system throughput, meaning the number of requests or messages the system can process per unit time. Each container is assigned CPU and memory requests and limits; when a service exceeds its allocated CPU limit, the operating system enforces CPU throttling, intentionally slowing execution to protect shared infrastructure. While throttling prevents resource starvation across services, it can reduce processing speed, increase queue backlogs, and raise latency, which is why system designers must balance scaling, partitioning, and rate limiting to maintain stable throughput under high load. Together, containers provide portability, Kubernetes provides orchestration and scaling, and resource controls like throttling ensure fair and predictable performance in real-world production systems."

5.4 Monolith vs Microservices (Interview version)

"A monolithic architecture packages the entire backend application - including APIs, business logic, and supporting features - into a single deployable unit that runs as multiple identical instances behind a load balancer, making development and deployment simpler in early stages. In contrast, a microservices architecture decomposes the backend into independently deployable services, each responsible for a specific business capability and typically running in its own container, allowing teams to scale, deploy, and maintain services independently. While monoliths offer lower operational complexity, simpler debugging, and efficient intra-process communication, they become harder to scale and evolve as systems grow because all components must be deployed and scaled together. Microservices are preferred in large real-world systems because they enable independent scaling of high-traffic components, better fault isolation, faster team autonomy, and safer incremental deployments, even though they introduce added complexity such as network communication, distributed coordination, and operational overhead. As a result, many organizations start with a monolith and gradually evolve toward microservices only when scale and organizational needs justify the transition."

5.5 Rate Limiting (Interview version)

"Rate limiting is a mechanism used to control how many requests a client or service can make within a given time window in order to protect system stability, ensure fair usage, and prevent overload or abuse. It is typically enforced at the API gateway, load balancer, or service layer using algorithms such as token bucket, leaky bucket, fixed window, or sliding window depending on whether burst tolerance or strict fairness is required. In distributed systems, rate limits are coordinated across multiple service instances using shared stores like Redis, gateway-managed policies, or hybrid local-global approaches to maintain consistency. By restricting excessive traffic, rate limiting prevents cascading failures such as CPU saturation, thread exhaustion, database overload, and queue backlogs, making it a critical reliability mechanism in large-scale microservice architectures."
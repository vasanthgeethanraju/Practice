When a client (web or mobile) makes an API call - typically a REST request such as GET /users/:id or POST /orders - the domain is first resolved via DNS into an IP address, which today commonly points to a nearby CDN edge location. The HTTPS request travels across internet hops to this CDN, where cached responses or static assets may be served immediately; on a cache miss, the request is forwarded to the origin infrastructure. It then reaches a reverse proxy or load balancer, which performs TLS termination, basic protection, health checks, and distributes traffic across horizontally scaled application servers running identical backend containers. Inside the application layer, an API router maps the endpoint to a controller or handler that validates headers, authentication, and payloads, and invokes the service layer where business logic executes. Before accessing persistent storage, the service checks an in-memory cache such as Redis using a cache-aside pattern; cache hits return data quickly, while misses trigger database queries and subsequently rebuild the cache with a TTL to reduce future load. The data access layer interacts with the database tier, where SQL databases are used when strong consistency and relational modeling are required (ACID workloads like payments and orders), while NoSQL systems support flexible schema and high-throughput distributed workloads such as feeds or logs, often accepting eventual consistency. For scalability, data may be horizontally partitioned through sharding using range or hash keys, with each shard maintaining a primary node for writes and replica nodes for reads via WAL or binlog replication. Performance is further improved through indexing (e.g., B-trees), vertical partitioning of hot versus cold data, and denormalization to reduce expensive joins, especially across shards. Large media assets are stored in blob or object storage with only metadata or URLs stored in databases, allowing clients to fetch media directly through CDN delivery. Once processing completes, the response flows back through the service and API layers to the load balancer and CDN, where cacheable responses may be stored at the edge for future requests, before finally returning to the client over the same HTTPS connection. Together, CDN caching, load balancing, horizontal scaling, and application-level caching progressively reduce latency and protect downstream systems, enabling modern distributed applications to scale efficiently under high traffic.
4.1 Scaling (Vertical vs Horizontal Scaling) (Interview version)

"Vertical scaling means increasing the capacity of a single machine by adding more CPU, memory, or resources, which is simple to implement but limited by hardware constraints and creates a single point of failure. Horizontal scaling, on the other hand, involves running multiple identical application instances and distributing incoming requests across them using a load balancer. This improves system capacity and availability because traffic is shared across instances and failures of individual nodes do not bring down the system. However, horizontal scaling requires the application to be largely stateless and introduces additional considerations like load balancing, shared storage, and distributed coordination."

4.2 Load Balancers (Interview version)

"A load balancer is a component that sits at the edge of an infrastructure between clients and application servers and distributes incoming requests across multiple identical backend instances to enable horizontal scaling, efficient resource utilization, and high availability. Typically implemented as a reverse proxy, it acts as a single entry point, performing request distribution using algorithms such as round robin, least connections, or weighted routing while continuously running health checks to ensure traffic is only sent to healthy servers. Load balancers operate either at Layer 4 (transport level, routing based on TCP/IP information) or Layer 7 (application level, where HTTP headers and paths can be inspected for intelligent routing), and they often terminate TLS to offload encryption from application servers, though some systems use re-encryption or TLS pass-through for stronger security. Because applications are horizontally scaled, load balancers assume stateless backend services, avoiding reliance on sticky sessions except in special cases. To eliminate single points of failure, production systems deploy multiple load balancers in active-active or active-passive configurations, with DNS distributing users across them despite DNS caching limitations controlled by TTL. In modern architectures, load balancing occurs hierarchically - DNS balances load balancers, load balancers balance application instances, and applications access shared data layers. Common real-world implementations include cloud-managed solutions like AWS Application Load Balancer (ALB) and Google Cloud Load Balancer, as well as self-managed proxies such as NGINX, HAProxy, and Envoy, often combined with edge providers like Cloudflare for global traffic distribution."

4.3 Caching (Interview version)

"Caching is a performance optimization technique where frequently accessed or computationally expensive data is stored in a fast in-memory layer, typically using systems like Redis or Memcached, so applications can serve requests without repeatedly querying the database. In modern architectures, caching usually sits between the application layer and the database, though it can also exist at multiple layers such as CDN edge caches, reverse proxy caches, and application-level caches. The most common approach is the cache-aside pattern, where the application first checks the cache, fetches from the database on a miss, and then stores the result with a TTL to limit staleness. Systems primarily cache read-heavy and stable data using patterns such as entity caching (individual objects like users or products), selective query caching for popular searches, and computed or aggregation caching for expensive operations like dashboards. While caching significantly reduces latency and database load and enables systems to scale read traffic efficiently, it introduces challenges including stale data, cache invalidation complexity, cache stampedes, hot keys, and cache pollution. These issues are mitigated through strategies like delete-on-write invalidation, TTL expiration, request locking or single-flight rebuilds, randomized expirations, background refresh, and careful cache key design using namespaces and versioning. In real-world systems, companies typically combine multiple caching layers—for example, CDNs like Cloudflare or CloudFront at the edge, reverse proxy caches such as NGINX or Varnish, and distributed in-memory stores like Redis or Memcached protecting the database. Overall, caching trades perfect consistency for massive performance gains and is a foundational technique used by large-scale platforms such as Amazon, Netflix, and Bloomberg to handle high read traffic while maintaining scalable and resilient architectures."

4.4 CDN (Interview version)

"A CDN, or Content Delivery Network, is a globally distributed network of edge servers that cache and serve content closer to users to reduce latency and offload traffic from origin infrastructure. Instead of every request reaching the application servers, CDN edge locations store cached copies of HTTP responses such as images, static assets, and cacheable API responses, allowing requests to be fulfilled locally on a cache hit. On a cache miss, the CDN forwards the request to the origin system—typically behind load balancers and application servers—then caches the returned response using TTL policies for future requests. CDNs significantly improve performance, scalability, and resilience by reducing bandwidth usage, protecting backend services from traffic spikes, and absorbing large volumes of requests or attacks. While they are less effective for fully personalized data, modern systems still leverage CDNs to cache shared page components and static resources, allowing only dynamic user-specific requests to reach backend services. Common real-world CDNs include Cloudflare, AWS CloudFront, Akamai, Fastly, and Google Cloud CDN, and they operate through geographically distributed physical edge locations called Points of Presence that serve content from locations nearest to users."